# Claims Reporting Automation System  
A cloudâ€‘native simulation of a real enterprise workflow: **mainframe output â†’ report generation (Excel/PDF) â†’ cloud upload â†’ ML-powered predictions â†’ dashboard for download**.

This project modernizes a legacy reporting pipeline using **FastAPI, React, GCP Cloud Run, Cloud Storage, PostgreSQL, and Python automation** â€” all built using **mock data** (fully safe and compliant).

---

## ğŸš€ Features

### ğŸ”¹ Endâ€‘toâ€‘End Automated Reporting  
- Upload mock claim files (CSV/TXT)  
- Parse, clean, and validate data  
- Generate **Excel + PDF** reports  
- Upload reports to **Google Cloud Storage**  
- View and download reports from a modern UI  

### ğŸ¤– ML-Powered Claims Prediction
- **Predict claim outcomes** (Paid/Denied) for pending claims
- **Random Forest** classification model
- **Probability scores** and confidence levels
- **Interactive dashboard** with visual predictions
- Real-time processing via React frontend

### ğŸ”¹ Cloudâ€‘Native Architecture  
- Backend: **FastAPI** on Cloud Run  
- Frontend: **React** dashboard with Tailwind CSS
- Storage: Cloud Storage buckets  
- Database: Cloud SQL (PostgreSQL) or SQLite (default)
- Scheduled jobs via Cloud Scheduler  

### ğŸ”¹ Enterpriseâ€‘Grade Modules  
- Clean folder architecture  
- ML model integration
- RESTful API design
- Error handling and validation
- CORS-enabled for frontend integration  

---

## ğŸ“¸ Screenshots

### ML Claims Prediction Dashboard

![Claims Prediction UI](screenshots/Claims-Reporting-Automation-System.png)

*Interactive dashboard showing ML predictions for pending claims with probability scores and confidence levels.*

### API Docs
![FastAPI interactive documentation](screenshots/Claims-Reporting-Automation-System-Swagger-UI.png)
   *- Shows the `/docs` endpoint*

---

## ğŸ§± Architecture Overview  

Client (React)  
â†’ Backend API (FastAPI)  
â†’ Data Processing (pandas)  
â†’ Report Generators (Excel/PDF)  
â†’ Cloud Storage (GCP)  
â†’ Database (PostgreSQL)

### High-Level Workflow  
```
Upload File â†’ Parse & Clean â†’ Aggregate Data 
â†’ Excel/PDF Generation â†’ Upload to GCP â†’ Metadata Saved 
â†’ Dashboard Download

ML Prediction Flow:
Upload CSV â†’ Feature Engineering â†’ Train Model 
â†’ Predict Pending Claims â†’ Display Results in Dashboard
```

---

## ğŸ› ï¸ Tech Stack

### Frontend  
- React  
- Tailwind CSS  
- Axios  
- Vercel / Cloud Run deployment

### Backend  
- FastAPI  
- Python (pandas, numpy, scikit-learn, openpyxl, reportlab)  
- SQLAlchemy  
- Pydantic  
- Google Cloud Storage SDK
- Machine Learning (Random Forest Classifier)

### Cloud  
- Cloud Run  
- Cloud Storage  
- Cloud SQL  
- Cloud Scheduler  

---

## ğŸ“ Folder Structure  

```
Claims-Reporting-Automation-System/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ reports.py         # API endpoints (upload, predict, list)
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â””â”€â”€ claims_predictor.py # ML model for predictions
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ report.py          # SQLAlchemy Report model
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ base.py            # Database base
â”‚   â”‚   â””â”€â”€ session.py        # Database session
â”‚   â””â”€â”€ services/
â”‚       â”œâ”€â”€ excel_generator.py # Excel/PDF report generation
â”‚       â”œâ”€â”€ ml_service.py      # ML service wrapper
â”‚       â””â”€â”€ storage.py        # Google Cloud Storage
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ ClaimsPredictor.js  # ML prediction UI
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ reports.js         # API client
â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â””â”€â”€ index.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”œâ”€â”€ sample_data/
â”‚   â””â”€â”€ claims_export_2025.txt # Sample claims data
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ§ª Sample Mock Data

Sample data is located in `sample_data/claims_export_2025.txt`:

```csv
ClaimID,Status,Amount,Date,Type
C001,Paid,550.25,2025-01-11,Medical
C002,Denied,200.00,2025-01-14,Dental
C003,Pending,130.50,2025-01-17,Vision
C004,Paid,1250.75,2025-01-18,Medical
...
```

**Required Columns:**
- `ClaimID`: Unique identifier
- `Status`: One of "Paid", "Denied", or "Pending"
- `Amount`: Numeric claim amount
- `Date`: Date in YYYY-MM-DD format
- `Type`: Claim type (e.g., "Medical", "Dental", "Vision")

---

## ğŸ”Œ API Endpoints  

### Reports  
- `POST /reports/upload` - Upload CSV file, generate Excel/PDF reports, upload to GCS
- `POST /reports/predict` - Upload CSV file, get ML predictions for pending claims
- `GET /reports` - List all processed reports

### System  
- `GET /health` - Health check endpoint

### Interactive API Documentation
- `GET /docs` - Swagger UI (FastAPI auto-generated docs)
- `GET /redoc` - ReDoc documentation  

---

## ğŸ“œ Environment Variables  

Create `.env` file in project root:

```env
# Database (optional - defaults to SQLite)
DATABASE_URL=postgresql://user:pass@host:port/db

# Google Cloud Storage (required for file uploads)
GCP_BUCKET_NAME=your-bucket-name

# Frontend API URL (optional)
REACT_APP_API_URL=http://localhost:8000
```

**Note:** See `SETUP_ENV.md` for detailed setup instructions.

---

## â–¶ï¸ How to Run Locally  

### Prerequisites
- Python 3.11+ with virtual environment
- Node.js 16+ and npm
- (Optional) PostgreSQL for production-like setup

### Backend Setup

1. **Create and activate virtual environment:**
   ```bash
   python -m venv venv
   venv\Scripts\activate  # Windows
   # or
   source venv/bin/activate  # Linux/Mac
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables:**
   - Copy `.env.example` to `.env` (if exists)
   - Set `DATABASE_URL` (optional - defaults to SQLite)
   - Set `GCP_BUCKET_NAME` (required for file uploads)

4. **Start the server:**
   ```bash
   uvicorn app.main:app --reload
   ```
   
   Server runs at: `http://localhost:8000`
   - API Docs: `http://localhost:8000/docs`
   - Health Check: `http://localhost:8000/health`

### Frontend Setup

1. **Navigate to frontend directory:**
   ```bash
   cd frontend
   ```

2. **Install dependencies:**
   ```bash
   npm install
   ```

3. **Start development server:**
   ```bash
   npm start
   ```
   
   Frontend runs at: `http://localhost:3000`

### Quick Test

1. Start backend: `uvicorn app.main:app --reload`
2. Start frontend: `cd frontend && npm start`
3. Open `http://localhost:3000`
4. Upload `sample_data/claims_export_2025.txt` for ML predictions

---

## â˜ï¸ Deployment

### Backend on Cloud Run  
- Build Docker image  
- Push to Container Registry  
- Deploy with Cloud SQL + Cloud Storage permissions  

### Frontend  
- Deploy to Vercel or Cloud Run  

### Scheduler  
- Cloud Scheduler â†’ calls `/reports/generate` on cron  

---

## ğŸ“š Documentation

- **[ML Integration Guide](ML_INTEGRATION_GUIDE.md)** - Complete guide for ML prediction feature
- **[Environment Setup](SETUP_ENV.md)** - Detailed environment variable configuration
- **[Virtual Environment Guide](VENV_GUIDE.md)** - Python virtual environment setup

## ğŸ“Œ Features & Capabilities

### Current Features
âœ… CSV/TXT file upload and validation  
âœ… Excel and PDF report generation  
âœ… Google Cloud Storage integration  
âœ… ML-powered claims prediction  
âœ… Interactive React dashboard  
âœ… RESTful API with auto-generated docs  
âœ… SQLite (default) or PostgreSQL support  

### Future Enhancements  
- [ ] Model persistence (save/load trained models)
- [ ] Prediction history in database
- [ ] Batch processing for large files
- [ ] Export predictions as CSV
- [ ] Role-based access control  
- [ ] Email notifications  
- [ ] Additional file formats  
- [ ] Multi-file parallel processing  

---

## ğŸ“„ License  
MIT License

---

## ğŸ¤ Contributions  
PRs and improvements are welcome!

---

## ğŸ§ª Testing the ML Prediction

1. **Start both servers** (backend and frontend)
2. **Upload sample data:**
   - Go to `http://localhost:3000`
   - Click "Select CSV/TXT File"
   - Choose `sample_data/claims_export_2025.txt`
   - Click "Predict Claims"
3. **View results:**
   - See summary statistics
   - Review predictions table with probabilities
   - Check denial risk scores

## ğŸ”§ Troubleshooting

### Common Issues

**Import errors (sklearn, numpy):**
```bash
pip install scikit-learn==1.3.2 numpy==1.26.4
```

**CORS errors:**
- Backend includes CORS middleware for `localhost:3000`
- Check `app/main.py` for CORS configuration

**File not found:**
- Ensure `sample_data/claims_export_2025.txt` exists
- Check file path in ML predictor script

**Frontend connection errors:**
- Verify backend is running on port 8000
- Check `REACT_APP_API_URL` in frontend `.env`

See [ML_INTEGRATION_GUIDE.md](ML_INTEGRATION_GUIDE.md) for detailed troubleshooting.

## â­ Acknowledgment  
This project is a modern cloud implementation inspired by the typical **mainframe â†’ reporting â†’ SharePoint** workflow used in enterprise environments, redesigned using modern engineering practices for learning and demonstration.

---

**Built with:** FastAPI â€¢ React â€¢ Tailwind CSS â€¢ scikit-learn â€¢ pandas â€¢ PostgreSQL/SQLite â€¢ Google Cloud Storage
